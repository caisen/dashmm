\documentclass[11pt]{book}
\usepackage[margin=1in]{geometry}
\usepackage[english]{babel}
\usepackage{hyperref}
\usepackage{verbatim} 
\usepackage{fontspec}
\defaultfontfeatures{Mapping=tex-text}
\setromanfont[Mapping=tex-text]{Palatino Linotype}

% Book's title and subtitle
\title{
\Huge 
\textbf{DASHMM 1.0.0 User Guide}
}
% Author
\author{
\textsc{Jackson DeBuhr}\thanks{\url{jdebuhr@indiana.edu}} \\
\textsc{Bo Zhang} \thanks{\url{zhang416@indiana.edu}}
}
\date{} 

\begin{document}

\frontmatter
\maketitle

\tableofcontents
%\listoffigures
%\listoftables

\mainmatter

\chapter{Introduction to DASHMM}

The {\it Dynamic Adaptive System for Hierarchical Multipole Methods}
(DASHMM) is a C++ library providing a general framework for
computations using multipole methods. In addition to the flexibility
to handle user-specified methods and expansions, DASHMM includes
built-in methods and expansions, including the Barnes-Hut~(BH) and
Fast Multipole Method~(FMM), and expansions implementing the Laplace
and Yukawa kernels. 

DASHMM is designed to make its adoption and use as easy as possible,
and so the interface to DASHMM provides both an easy-to-use basic
interface and a more advanced interface. The basic interface allows a
user to get up and running with multipole methods as quickly as
possible. However, more advanced use-cases, especially for users that
are implementing their own methods and expansions, will need to
explore the advanced interface. Finally, for the truly interested, the
entire library reference documentation can be found at the end of this
documentation. 

DASHMM is built using the advanced runtime system, HPX-5, but the
basic, and much of the advanced interface does not require any
knowledge of how to use HPX-5 directly. Instead, DASHMM insulates the
users from the specific details of HPX-5, allowing expression of the
multipole method application in higher level concepts than the
specifics of threads and execution control structures. Nevertheless,
it can be helpful to have a sense of the conceptual underpinnings of
HPX-5, and how those relate to DASHMM. Information about this can be
found in this guide in both the basic and advanced interface. Even
more information can be found at the HPX-5 website
(\url{https://hpx.crest.iu.edu/}). 

For a description of the techniques that go into the parallel
execution of DASHMM, and for a discussion of the conceptual framework,
please see the code paper: {\it ``DASHMM: Dynamic Adaptive System for
  Hierarchical Multipole Methods''} in Communications in Computational
Physics, Vol. 40 (2016), No. 4, pp. 1106-1126. 

This document covers version 1.0.0 of DASHMM. For the latest news and
updates of DASHMM, please visit the DASHMM website:
\url{https://www.crest.iu.edu/projects/dashmm/.} 

The DASHMM project has adopted semantic versioning. For a description,
please see \url{http://semver.org}, and the chapters on the basic and
advanced use of DASHMM should be considered to be the specification of
the interface to the library. The more complete documentation, which
has been automatically generated by Doxygen, should be considered in
many instances to be implementation details. 

In the following, snippets of code or the names of code constructs
will be set in a fixed width font. For example, {\tt main()}. Unless
otherwise indicated, every construct presented in this guid is a
member of the {\tt dashmm} namespace. 


\chapter{Installing DASHMM} 

This chapter outlines what is needed to install and use of DASHMM, and
gives a brief sketch of the demo programs included with DASHMM. 

\section{Prerequisites} 

DASHMM v. 1.0.0 depends on one external library: HPX-5. The current
version of DASHMM depends on version 3.2.0 of HPX-5 or later, which
may be found at \url{https://hpx.crest.iu.edu/}. Please see the
official HPX-5 documentation for instructions on how to build, install
and run HPX-5 on your system. 

The DASHMM build system relies on the {\tt pkg-config} utility to
specify the needed HPX-5 compilation and linking options, so it is
important to add the correct path for HPX-5 to your {\tt
  PKG\_CONFIG\_PATH} environment variable. For example, assuming HPX-5
is installed in {\tt /path/to/hpx}, this can be accomplished with {\tt
  bash} with: 

\begin{verbatim} 
export PKG_CONFIG_PATH=/path/to/hpx/lib/pkgconfig:$PKG_CONFIG_PATH
\end{verbatim} 

\section{Building DASHMM} 

The DASHMM library is straightforward to build. Once the previous
prerequisite is met, one needs to perform the following steps: 

\begin{enumerate} 
\item Unpack the source code into some convenient directory. For the
  sake of discussion, this guide assumes that the code has been
  unpacked in {\tt /path/to/dashmm/}. 
\item In {\tt /path/to/dashmm/} can be found {\tt Makefile}. There are
  few (if any) changes that need to be made to this file for
  successful compilation. The most likely change would be to modify
  the compiler used. Any compiler supported by HPX-5 will be able to
  compile DASHMM, provided it also support the C++11 standard. For
  example, to change the compiler from the default ({\tt g++}) to the
  Intel compiler, one needs to replace {\tt CXX=g++} with {\tt
    CXX=icpc}. 
\item Run {\tt make} from {\tt /path/to/dashmm}. This should build the
  library statically and it will be ready to link with your specific
  application. 
\end{enumerate} 

Because DASHMM is heavily templated, a good deal of the code is
contained in the header files. This means that much of the compilation
of the DASHMM code will occur when the user code is compiled. This can
increase the compilation time of user code, but the added flexibility
of DASHMM is worth the minor increase in compilation time. 

\section{Linking against DASHMM} 

To build a program using the DASHMM library, only a few things need to
be done. DASHMM builds in place, so when compiling codoe that used the
library, one must specify where to look for the header files, and
where to look for the built library. Further, because DASHMM relies on
HPX-5, one must also specify how to find HPX-5. For HPX-5 this is the
easiest with the {\tt pkg-config} utility. 

Assuming that DASHMM was built in {\tt /path/to/dashmm}, to compile
code (with, for example {\tt g++}) one must specify the following
arguments for compilation: 

\begin{verbatim} 
-I/path/to/dashmm/include $(shell pkg-config --cflags hpx)
\end{verbatim} 

Similarly, one must specify the following arguments for linking: 

\begin{verbatim}
-L/path/to/dashmm/lib -ldashmm $(shell pkg-config --libs hpx)
\end{verbatim}

Examples of this can be found in the demo programs included with
DASHMM. 

\section{DASHMM demo programs} 

Included with DASHMM are several test codes that demonstrate the use
of the library. These are found in the {\tt /path/to/dashmm/demo}
subfolder. Each test can be built simply by running {\tt make} in the
code's directory. Detailed information about each code can be found in
the provided {\tt README} file. 

\subsection{Basic demo} 
The {\tt basic} demo code creates a random distribution of source and
target points and computes the potential at the targets due to the
sources using any of the built-in kernels provided with DASHMM. A user
can request a summary of the options to the test code by running the
code with {\tt --help} as a command line argument, or by reading {\tt
  /path/to/dashmm/demo/basic/README}. 

\subsection{Time-stepping demo} 
The {\tt stepping} demo code creates a distribution of particles and
computes their acceleration and integrates their motion forward in
time. Note that the point of the demo is not to provide a great time
integrator, so many fine points of creating a good integrator are
skipped. Instead, {\tt stepping} demonstrates those features of DASHMM
that enable time-stepping codes as a use-case for DASHMM. A user can
request a summary of the options to the code by running the code with
{\tt --help} as a command line argument, or by reading {\tt
  /path/to/dashmm/demo/stepping/README}. 

\subsection{User-defined expansion demo} 

The {\tt user} demo provides a skeleton code that implements a new
expansion type. The code is documented, and the requirements of the
Expansion concept are outlined in the in-source comments. This example
should be considered to be more advanced, and would require
investigation of the advanced interface to understand completely. 

\chapter{Basic guide to DASHMM} 
In this chapter, the basic interface to DASHMM will be
covered. Generally speaking, the basic user interface to DASHMM is
anything needed to employ the provided methods and kernels in
application. For instructions on how to define and use new methods and
kernels, please see the next chapter on the advanced user interface. 

One of the goals of DASHMM is to make it easy to use multipole
methods, and so the basic interface targets ease-of-use. Additionally,
the library aims to make it easy to perform parallel computations
using the multipole method. However, the advanced dynamic techniques
that DASHMM employs are not simple to use directly, so the library is
constructed in a way that allows the user to get the benefits of
parallel execution without having to write the parallel code
themselves. That being said, it can be useful to have a sense of what
underlies the conceptual framework of DAHSMM. And so, the following
section will cover these concepts at a level that might be relevant
for basic use of the library. More details can be found in the next
chapter. 

\section{DASHMM concepts} 
This section covers both the conceptual framework of the multipole
methods supported by DASHMM and the paralellization of those
methods. More explanation of DASHMM's conceptual framework can be
found in the following chapter, or in the code paper. 

\subsection{Multipole method abstractions} 

DASHMM is a templated library allowing for the description of a
general set of multipole methods with a small set of template
parameters. To use DASHMM, one must specify four types: the {\it Source}
data type, the {\it Target} data type, the {\it Expansion} type and
the {\it Method} type. Within certain limits, each of these can be
varied independently, for example, a given expansion might be used
with a number of methods, allowing users to easily experiment and
select the method that most meets their needs. 

DASHMM includes a number of built-in expansion and method types. For
more, see below. 


\subsubsection{Source} 
The Source type gives the structure of the source point data. There
are few requirements on the Source type. Typically the minimum
requirements are the position and charge of the source. The {\tt
  position} is of type {\tt Point} and the {\tt charge} is of type
{\tt double}. For example, the following is a minimal Source type that
works with every expansion provided with DASHMM. 

\begin{verbatim} 
struct SourceData {
  Point position; 
  double charge; 
}; 
\end{verbatim} 

Beyond these required members, anything might be added to a Source
type. This allows the user to associate application specific data to
the sources in the evaluation. The only additional requirement is that
the resulting type be trivially copyable. 

\subsubsection{Target} 
The Target type gives the structure of the target point data. There
are similarly few requirements on the Target type. Each Target type
will need a {\tt position} of type {\tt Point} and a member to store
the result. The details on the exact requirements can be found in the
documentation of the individual expansions below. Typically this is a
member {\tt phi} of type {\tt std::complex<double>}, which been
  aliased as {\tt dcomplex\_t} in DASHMM. So the following would work
  for many of the included DASHMM expansions: 


\begin{verbatim} 
struct TargetData {
  Point position; 
  std::complex<double> phi;
}; 
\end{verbatim} 

Beyond the required members, anything might be added to the Target
type. A typical choice is an identifier of some kind because DASHMM
evaluations will sort the input data to suit the parallel computation,
and points can then be identified after the computation. Like with the
Source type, the resulting Target type must be trivially copyable. 

\subsubsection{Expansion} 

The particular potential or interaction that is being computed with
the multipole moment is called the kernel. For example, the Laplace
kernel is the traditional potential from electrostatics or Newtonian
gravitation. In DASHMM, kernels are not represented directly. Instead,
Expansions are created that implement the needed operations for the
particular kernel. The distinction is that there are often multiple
ways to expand a given kernel to be used in a multipole method
computation. Each Expansion represents a way (or a closely related set
of ways) that the potential is expanded into an approximation. 

The Expansion type is a template type over two parameters, the source
and target types being employed. It is the Expansion type that places
restrictions on the Source and Target types; the Expansion requires
certain data to compute from (taken from the Sources), and it will
produce certain data (into the Targets). This allows the Expansion to
operate on any data that meets its requirements, meaning that a very
general set of source and target data types can be supported. 

For details on creating user-defined Expansions, please see the
advanced use guide in the next chapter. This will also include the set
of operations that an Expansion must support. 

\subsubsection{Method} 
The final major abstraction in DASHMM is the Method. This type
specifies how the Expansion is used on the provided Source data to
compute the interaction at the locations specified in the Target
data. The Method is responsible for connecting the various Expansions
representing the hierarchically subdivided set of Source and Target
locations with the appropriate operations provided by the
Expansion. It is the Method that allows one to perform both a
Barnes-Hut computation as well as the Fast Multipole Method. 

The Method type is a template over three types: the Source, the Target
and the Expansion. For the Method, it is not important exactly how the
various operations are implemented, but only that they are
implemented. The details of the Expansion, and thus the central
details of the particular interaction being studied, are hidden and
unimportant to the method. 

Despite this generality, it is possible to create a Method that only
works for certain expansions. Indeed, included in DASHMM is the {\tt
  LaplaceCOM} expansion, which does not provide implementations for
all of the operations needed by the {\tt FMM} or {\tt FMM97}
methods. This is intentional, as the style of expansion in {\tt
  LaplaceCOM} is not terribly well suited to the Fast Multipole
Method. Nevertheless, great flexibility and generality is possible
with DASHMM. 

For details on creating user-defined Methods, please see the advanced
user guide in the next chapter. This will also include a description
of the set of routines that make up the method concept. 

\subsection{Parallelization abstractions} 
DASHMM uses the advanced runtime system HPX-5 for its
parallelization. HPX-5 provides a number of features that allow DASHMM
to naturally express the parallelism and data dependence of the
computation directly in programming constructs. However, the
flexibility of HPX-5 comes with a significant amount of effort to
learn the system. One major goal of DAHSMM is to get the benefits of
the dynamic adaptive techniques enabled by HPX-5 without the end-user
having to write directly to HPX-5 constructs, and DASHMM is successful
in this regard. It is, nevertheless, useful to have some notion of a
few concepts from HPX-5 for the basic use of DASHMM. For more details
on HPX-5, please visit \url{https://hpx.crest.iu.edu/}. For more
details on the use of HPX-5 in DASHMM, please see the advanced use
guide in the next chapter, or the code paper. 

\subsubsection{PGAS} 
HPX-5 provides a partitioned global address space (PGAS) that DASHMM uses for
the data during the computation.

That the address space is partitioned means that though the addresses are
unified into a global space, each byte of the global address space is served
by the physical memory on one particular locality of the system. A locality
is similar to the concept of a rank in an MPI program. That the address space
is global means that there is a single virtual address space allowing any
locality to refer to data, even if that data is not stored in the same physical
memory of the referring locality.

Practically speaking for users of DASHMM, the important part of the global
address space provided by HPX-5, as used by DASHMM, is that it is partitioned.
Each locality will have direct access to a portion of the data, and more
indirect access to all of it.

\subsubsection{Execution model} 
The execution model for a DASHMM program is one very similar to the SPMD model.
The program written to use DASHMM will be run on every locality in the allocated
resources. And at certain points, the execution will be handed off to DASHMM and
ultimately HPX-5 by making DASHMM library calls. Inside DASHMM, the execution is
very dynamic and is formed of a large number of small, interdependent tasks.
This complication, however, is hidden from the user. Instead, DASHMM presents
an interface where each locality participates in collective calls, with each
presenting possibly different data to the DASHMM library. So in many ways,
using basic DASHMM will be very similar to using MPI.

\section{Basic types} 
DASHMM defined a number of basic types that are used throughout the system,
and which might be used by users of the library.

\subsection{{\tt ReturnCode}}

DASHMM calls will return values of the type {\tt ReturnCode} where it is
reasonable to do so. The possible values are: {\tt kSuccess}, {\tt
  kRuntimeError}, {\tt kIncompatible},
{\tt kAllocationError}, {\tt kInitError}, {\tt kFiniError} and 
{\tt kDomainError}. See specific
library calls for cases where each might be returned.

\subsection{{\tt dcomplex\_t}}
Many kernels return potential values that are complex numbers. DASHMM provides
{\tt dcomplex\_t} an alias to {\tt std::complex<double>}. 

\subsection{{\tt Point}}
The {\tt Point} class is used to represent locations in three dimensional space.
{\tt Point} is expected for giving the locations of sources and targets. It has
the following members:

\begin{itemize} 
 \item {\tt Point::Point(double x = 0, double y = 0, double z = 0)}: Construct a
 point from a given set of coordinates. This can also be used to default
 construct a point.
 \item {\tt Point::Point(double *arr)}: Construct a point from a C-style
   array. It is  important that {\tt arr} should contain at least three members.
 \item  {\tt Point::Point(const Point \&pt)}: Copy construct a point.
 \item {\tt Point Point::scale(double c) const}: Return a point whose
   coordinates have  all been scaled by the factor {\tt c}.
 \item {\tt double Point::operator[](size\_t i) const}: Indexing access to the
 coordinates of the point. {\tt i} must be in the range $[0,2]$.
 \item  {\tt double Point::x() const}: Return the x coordinate of the point.
 \item  {\tt double Point::y() const}: Return the y coordinate of the point.
 \item {\tt double Point::z() const}: Return the z coordinate of the point.
 \item  {\tt double Point::norm() const}: Return the 2-norm of the point.
 \item {\tt void Point::lower\_bound(const Point \&other)}: This computes the
   lowest  coordinate in each direction of this point and {\tt other} and sets
   this point's  coordinate to that value.
 \item {\tt void Point::upper\_bound(const Point \&other)} : This computes the
   highest coordinate in each direction of this point and `other` and sets this
   point's  coordinates to that value.
\end{itemize} 

Additionally, there are a few non-member operations that are defined:

\begin{itemize}
 \item {\tt double point\_dot(const Point \&left, const Point \&right)}: Treat
   the  points as if they are vectors and take their dot product.
 \item {\tt Point point\_add(const Point \&left, const Point \&right)}: Perform
   a  component-wise addition of {\tt left} and {\tt right} and return a point
   with  the result.
 \item {\tt Point point\_sub(const Point \&left, const Point \&right)}: 
Perform a component-wise subtraction of {\tt right} from {\tt left} and return a
point  with  the result.
\end{itemize}

\section{Initializing DASHMM} 

DASHMM must be initialized and finalized to be used. There are some DASHMM
operations that must only occur before initialization, and some that can only
occur after initialization. All DASHMM operations must occur before the
library is finalized.

\subsection{{\tt ReturnCode init(int *argc, char ***argv)}}

Initialize the runtime system supporting DASHMM and allocate any resources
needed by DASHMM. The addresses of the command line arguments must be
provided as the behavior of HPX-5 can be controlled by these arguments. Any
arguments dealing with HPX-5 directly will be removed and {\tt argc} and 
{\tt argv} will be updated accordingly.

{\tt init()} returns {\tt kSuccess} if the system is successfully started, and
{\tt kRuntimeError} otherwise. If {\tt init()} returns {\tt kRuntimeError} all
subsequent calls to DASHMM will have undefined behavior.

All other DASHMM library calls must occur after {\tt init()}. However, some
DASHMM related objects must be constructed before the call to {\tt init()}. See
below for details.

This is a collective call; all localities must participate.

\subsection{{\tt ReturnCode finalize()}}

This will free any DASHMM specific resources and shut down the runtime system.
No other calls to DASHMM must occur after the call to {\tt finalize()}. This is
a collective call; all localities must participate.


\section{SPMD utilities} 

DASHMM provides a small number of traditional SPMD utilities to make certain
things simpler.

\subsection{{\tt int get\_my\_rank()}}

This returns the rank of the calling locality.

\subsection{{\tt int get\_num\_ranks()}}

This returns the number of ranks available.

\subsection{{\tt void broadcast(T *value)}}

This performs a broadcast of the given value at rank 0 to all other ranks.
This is a template over the type {\tt T}. This is a collective operation. Each
rank must provide the address of a type {\tt T} object. For rank 0, this will
provide  the address of the value to share; for all other ranks this provides
the address into which the value broadcast from rank 0 will be stored.

\section{Evaluation}

The central object in any DASHMM evaluation is the {\tt Evaluator} object. This
object not only manages the registration of certain actions with the runtime
system, but also provides the interface to performing the multipole method
evaluation.

The {\tt Evaluator} object is a template over four types: the source type, the
target type, the expansion type and the method type. The {\tt Evaluator} for a
given set of types must be declared before the call to {\tt init()}. For
example: 

\begin{verbatim}
dashmm::Evaluator<Source, Target, dashmm::Laplace, dashmm::FMM97> eval{};
\end{verbatim}

would be an {\tt Evaluator} for the Laplace kernel using the advanced FMM method
(see below for details) for two user-defined types {\tt Source} and {\tt Target}
implementing the data that the user requires of the source and target points. 

Specifying the full type of the evaluator will cause the template to expand
out all of the needed actions to actually implement the evaluation using
HPX-5, and will also register those actions with HPX-5.


\subsection{{\tt ReturnCode Evaluator::evaluate(...)}}

Perform a multipole method evaluation. The arguments to this method, in order,
are as follows:

\begin{itemize}
\item {\tt const Array<Source> \&sources}: the Array containing the source data.
\item {\tt const Array<Target> \&targets}: the Array containing the target data.
\item {\tt int refinement\_limit}: the refinement limit of the tree. The sources
  and  targets will be placed into a hierarchical partitioning of space. This
 partitioning will end when there are fewer sources or targets than the supplied
 refinement limit.
\item {\tt const Method<Source, Target, Expansion<Source, Target>> \&method}:
  the  method to use for the evaluation. A few methods require parameters at
 construction, so this is passed in to provide those parameters to the
 evaluation.
\item {\tt int n\_digits}: the accuracy parameter for the {\tt Expansion} in
  use. This is  often the number of digits of accuracy required.
\item {\tt const std::vector<double> \&kernelparams}: the parameters for the
  kernel evaluation. These are those quantities that are constant for each use
  of the particular expansion. For example, the strength of the electrostatic
  force when using the Laplace kernel is one example of a kernel parameters. See
  the individual expansions for details about what needs to be provided.
\end{itemize} 

Note that during evaluation, the records in the source and target arrays may be
sorted. So a separate identifier should be added to the source and target types
if the identity of the sources or targets needs to be tracked. Other than the
sorting, the only change to the data in the target array will be the output
potential or other field value (as specified by the chosen expansion). The
only change to the source array beyond the sorting will only occur in the case
that the source and target arrays are the same, in which case the previous
comment about the targets also applies to the source.

This is a collective call; all localities must participate.

The possible return values are {\tt kSuccess} when there is no problem, and
{\tt kRuntimeError} when there is a problem with the execution.

\section{DASHMM array} 
DASHMM provides an array construct that represents a collection of records in
the global address space provided by HPX-5. The {\tt Array} object is a template
type over the record. The only requirement on the record type is that it is
trivially copyable. When an {\tt Array} is created, as in

\begin{verbatim}
Array<T> source_data{};
\end{verbatim}

no global memory is yet allocated for the array. To allocate the global memory
that will serve the array, one must use {\tt allocate()} (see below for
details). Array objects can be thought of as a traditional array, but which is
broken into one part for each locality in the system. The parts, or segments of
the array, could have different lengths. Some segments can even have no records,
meaning that a given locality does not have a share of the data.

{\tt Array} objects are intended to be employed in user code, and so most
members of the interface are SPMD in nature, and are collective operations.

\subsection{{\tt Array<T>::Array()}}

{\tt Array} objects have a single constructor, which does not allocate any
global memory. It takes a single optional argument that is not needed for basic
use of DASHMM. When default constructed, an {\tt Array} will be invalid (see
{\tt valid()} below.)

This is not a collective operation. To use an array, each locality must create
an {\tt Array} object. The individual {\tt Array} objects will be bound into a
unified object referring to the global address space using {\tt allocate()}.

\subsection{{\tt bool Array<T>::valid() const}}

This returns if the array is valid, that is, it refers to some global memory.
When initially constructed, an {\tt Array} will be invalid, and can only be made
valid by allocating global memory for the records (see {\tt allocate()}).

This method is not collective.

\subsection{{\tt size\_t Array<T>::count() const}}

This returns the number of records in the segment of the array on the calling
locality. This is a collective call, and it is an error to call this method
on an invalid array. Each calling rank will receive a different result from
this method.

\subsection{{\tt size\_t Array<T>::length() const}}

This returns the total length of the entire array. The result of {\tt length()}
is equal to the sum of the results of {\tt count()} from each rank. This is a
collective call, and it is an error to call this method on an invalid array.

\subsection{{\tt ReturnCode Array<T>::allocate(size\_t record\_count)}}

This allocated the global memory to serve an array with the given counts on
each locality. After this call the array will be valid unless there is an
error, which will be indicated by the return code. Possible return values are:
{\tt kSuccess} if the array is successfully allocated; {\tt kDomainError} is the
object already has an allocation; {\tt kAllocationError} is the global memory
cannot be allocated; or {\tt kRuntimeError} if there is some error in the
runtime. 

This is a collective call. Each locality can provided a different number of
records to allocate via the {\tt record\_count} parameter. The resulting global
allocation will match the input of {\tt allocate()}. A locality can provide {\tt
  0} as an argument, so long as at least one rank asks for a non-zero number of
records. For instance, one locality might allocate all of the records for
an array, or each locality might own a portion of the overall records.

\subsection{{\tt ReturnCode Array<T>::destroy()}}

This will destroy the global memory allocated for this array object. It is
an error to call this on an invalid array. This is a collective call. The
result of the method will be either {\tt kRuntimeError} if there is an error
in the runtime, or {\tt kSuccess} otherwise.

\subsection{{\tt ReturnCode Array<T>::get(size\_t first, size\_t last, T *out)}}
This method gets data from an array object, and places the requested
records into the buffer provided by {\tt out}. The range of records that is
retrieved is specified by {\tt first} (inclusive) and {\tt last} (exclusive).

This is a collective call. Each locality will provide a different range of
records, and a different local buffer into which the retrieved data will be
placed. {\tt first} and {\tt last} are given in terms of the segment of the
array on this locality. To discover the number of records on the calling
locality, use {\tt count()}. 

Note that this is a copy of the global data; changes to the retrieved values
will not be reflected in the array object.

This method will return one of the following: {\tt kRuntimeError} if there is an
error with the runtime; {\tt kDomainError} if the provided index range is
inconsistent with the array object; or {\tt kSuccess} otherwise.

\subsection{{\tt ReturnCode Array<T>::put(size\_t first, size\_t last, T *in)}}

This method puts data into an array object, copying the specified
records from the buffer provided by {\tt in}. The range of records that is
copied is specified by {\tt first} (inclusive) and {\tt last} (exclusive).

This is a collective call. Each locality will provide a different range of
records, and a different local buffer from which the retrieved data will be
placed. {\tt first} and {\tt last} are given in terms of the segment of the
array on this locality. To discover the capacity of the array on the calling
locality, use {\tt count()}.

Note that this places a copy of the records into the global address space;
changes in the local data will not be reflected in the array object.

This method will return one of the following: {\tt kRuntimeError} if there is an
error with the runtime; {\tt kDomainError} if the provided index range is
inconsistent with the array object; or {\tt kSuccess} otherwise.

\subsection{{\tt T *Array<T>::collect()}}

This method collects all of the records in the array and returns a new local
allocation containing the records at locality 0. This is largely speaking a
convenience feature. Note that this will allocate a copy of the entirety of the
array on one locality.

This is a collective call. The returned pointer will only be valid on locality
zero. All other ranks will receive {\tt nullptr}.

Note that this provides a copy of the data; changes to the returned data will
not be reflected in the array object.

\section{Array map actions}

To avoid the round trip from and to the global address space via {\tt Array}'s
{\tt get()} and {\tt put()} methods, one can make use of the {\tt
  ArrayMapAction}  type.
This type specifies an action to be performed on the records of an array.
Then, together with a method of the {\tt Array} object, this allows for
some computation to occur on the records of an array.

This class is a template requiring two parameters: the type of records for
the array to which the action will be applied (hereafter {\tt T}), and a type
specifying an environment to provide to the action (hereafter {\tt E}).

The action is specified during construction of an object of type
{\tt ArrayMapAction}. This is provided as a function pointer, for a function
with a particular signature. So that DASHMM can use the action on every
locality, like the {\tt Evaluator} object, any {\tt ArrayMapAction} objects must
be defined before {\tt init()} is called.

The function implementing the action must take three arguments, the first is
a {\tt T *} giving the data on which to act, the second is a {\tt const size\_t}
gives the number of records on which to act, and the third is {\tt const E *},
where {\tt E} is the environment type of the {\tt ArrayMapAction}. For example:

\begin{verbatim}
void update_position(T *data, const size_t count, const E *env) {
  for (size_t i = 0; i < count; ++i) {
    data[i].position += data[i].velocity * env->delta_t;
  }
}
\end{verbatim}

is an action that might perform a position update for a time-stepping code.
It is important to note that the action implementation can place requirements
on the array record type {\tt T}. In the previous example, type {\tt T} needs to
have a member called {\tt velocity}. The first argument to the action will be a
correctly offset pointer into a contiguous chunk of records. The action should
only assume that {\tt data[0]} through {\tt data[count - 1]} are available for
use. 

\subsection{{\tt ArrayMapAction<T, E>::map\_function\_t}}

This class aliases the type of function that can implement the action. It is:

\begin{verbatim}
void (*)(T *, const size_t, const E *)
\end{verbatim}

\subsection{{\tt ArrayMapAction<T, E>::ArrayMapAction(map\_function\_t f)}}

This constructs an array map action object. The provided function pointer
will give the action that is performed on the array. To use an action,
the associated {\tt ArrayMapAction} must be defined before {\tt init()} is
called. This object will register the needed actions with the runtime system.

For any given action {\tt f} only a single {\tt ArrayMapAction}  
should be defined.

\subsection{{\tt ReturnCode Array<T>::map(ArrayMapAction<T, E> \&act, const E
    *env)}} 

Once an {\tt ArrayMapAction} is defined, it can be used on a specific array by
calling that array's {\tt map()} method. This will cause the action represented
by {\tt act} to be applied on all entrieds of this array. The action ultimately
works on segments of the array. The environment, {\tt env} is provided
unmodified to each segment.

This is a collective call. It is an error to {\tt map()} on an invalid array.

\section{Built-in methods}

DASHMM includes a number of built-in methods that are ready to use for
problems: the Barnes-Hut method, two forms of the Fast Multipole Method and
a Direct summation method. These will be covered in detail below. To
successfully compile, all operations in the Expansion concept need to be
implemented for a given expansion. However, some methods only require a
subset of the full complement of operations. These will be covered below,
in order of increasing complexity.

\subsection{{\tt Direct}}

The {\tt Direct} method is primarily intended for use as a comparison for
computing 
the exact result for a given set of sources. There is some parallelism
implemented for this method, so the cost can be reduced somewhat. However, for
realistic problem sizes, this method should be avoided.

The only operation that is used by the {\tt Direct} method is the 
{\tt S->T}, 
so any Expansion implementing that operator can be used with the {\tt Direct}
method.


Please see the demo program {\tt demo/basic} included with DASHMM for an example
use case of {\tt Direct} and note that it is only applied to a very small subset
of the target locations.

Construction of {\tt Direct} is simple, as it can only be default constructed.

\subsection{{\tt BH}}

The {\tt BH} method implements the Barnes-Hut algorithm in the framework of
DASHMM. The implemented method uses the simple multipole acceptance criterion
parameterized by a single angle. If a multipole expansion is centered a
distance $R$ away from a given point and the multipole expansion is
associated with a region of size $D$ then the multipole expansion is used
only if $D/R < \theta_C$ for some critical angle $\theta_C$.

In practice, since the
hierarchical partioning of the source and target locations in DASHMM produces
leaves that can contain multiple particles, the multipole acceptance criterion
is evaluated pessimistically: the radius is computed from the nearest possible
location in a given target tree node to the multipole in question. This will
tend to produce results with a slightly smaller error, with a slightly larger
execution time.

When creating a {\tt BH} object, the opening angle $\theta_C$ is specified:

\begin{verbatim}
BH bh_method(0.6);
\end{verbatim}

Generally the opening angle should be less than one. As the angle approaches
zero, the method approaches the direct summation method. The critical angle
for a particular {\tt BH} object can be accessed with {\tt double BH::theta()},
which returns the angle specified at creation time. A default constructed
{\tt BH} uses a critical angle of {\tt 0}.

In addition to the direct contribution, {\tt S->T}, to use {\tt BH} an expansion
must also implement fully the following operations: {\tt S->M}, {\tt M->M} 
and {\tt M->T}.

\subsection{{\tt FMM}}

The {\tt FMM} method implements the Fast Multipole Method in its original form.
To create an {\tt FMM} method, no arguments are needed as the decisions about
which expansions to use in which situations are all made based on fixed
geometric considerations.

The following operations must have a full implementation in an expansion to be
used with {\tt FMM}: {\tt S->T}, {\tt S->M}, {\tt S->L}, {\tt M->M}, 
{\tt M->L}, {\tt M->T}, {\tt L->L}, and {\tt L->T}.

\subsection{{\tt FMM97}}

The {\tt FMM97} method implements the Fast Multipole Method in the form that
uses exponential expansions and the merge-and-shift technique. No parameters
are needed to construct an {\tt FMM97} method.

The following operations must have a full implementation in an expansion to be
used with {\tt FMM97}: {\tt S->T}, {\tt S->M}, {\tt S->L}, {\tt M->M}, 
{\tt M->L}, {\tt M->T}, {\tt L->L},
{\tt L->T}, {\tt M->I}, {\tt I->I} and {\tt I->L}.

\section{Built-in expansions}

DASHMM includes a number of built-in expansion that are ready to use for
applications. Each expansion will have a different set of implemented
operations which will restrict their use for certain methods. Further, each
expansion will place requirements on the source and target types used during
a DASHMM evaluation. These details will be covered for each expansion
below.

\subsection{{\tt Laplace}}

The {\tt Laplace} expansion is a spherical harmonic expansion of the Laplace
potential. Unlike {\tt LaplaceCOM} and {\tt LaplaceCOMAcc} below, 
this expansion is
designed to handle sources with both signs of charge without losing accuracy.
This potential is scale-invariant, so there are no kernel parameters that are
needed in the call to {\tt evaluate()}. However, calls to evaluate must supply
an accuracy parameter giving the number of digits of accuracy that are
requested.

Though this expansion is in principle compatible with every method included
with DASHMM, it is designed for the {\tt FMM} and {\tt FMM97} methods.

This expansion imposes the following restrictions on the source type: a
member of type {\tt Point} with the name {\tt position} must be provided; a
member of 
type {\tt double} with the name {\tt charge} must be provided.

This expansion imposes the following restrictions on the target type: a
member of type {\tt Point} with the name {\tt position} must be provided; 
a member of type {\tt dcomplex\_t} with the name {\tt potential}
 must be provided.

\subsection{{\tt Yukawa}}

The {\tt Yukawa} expansion is a spherical harmonic expansion of the Yukawa
potential. This potential is scaling variant, so a single kernel parameter
must be provided to {\tt evaluate()}. Calls to evaluate must also supply an
accuracy parameter that gives the number of digits of accuracy required.

Though this expansion is in principle compatible with every method included
with DASHMM, it is designed for {\tt FMM97} method.

This expansion imposes the following restrictions on the source type: a
member of type {\tt Point} with the name {\tt position} must be provided; 
a member of
type {\tt double} with the name {\tt charge} must be provided.

This expansion imposes the following restrictions on the target type: a
member of type {\tt Point} with the name {\tt position} must be provided; 
a member of type {\tt dcomplex\_t} with the name {\tt potential}
 must be provided.

\subsection{{\tt LaplaceCOM}}

The {\tt LaplaceCOM} expansion is a center of mass expansion of the Laplace
potential. This form of the expansion extends to the quadrupole term, and
because of the choice of center has an identically zero dipole term. This
expansion can be used to compute the potential. See {\tt LaplaceCOMAcc} for an
equivalent expansion that computes the acceleration. This potential is
scale-invariant so not kernel parameters are needed in the call to 
{\tt evaluate()}. 
Further, the number of terms in the expansion is fixed, so the accuracy
parameter to {\tt evaluate()} is ignored.

This expansion is only compatible with the {\tt BH} or {\tt Direct} methods; 
it does
not implement all the needed operations for use with {\tt FMM} or {\tt FMM97}.

This expansion imposes the following restrictions on the source type: a
member of type {\tt Point} 
with the name {\tt position} must be provided; a member of
type {\tt double} with the name {\tt charge} must be provided.

This expansion imposes the following restrictions on the target type: a
member of type {\tt Point} with the name `position` must be provided; a member
of 
type {\tt dcomplex\_t} with the name {\tt potential} must be provided.

\subsection{LaplaceCOMAcc}

The {\tt LaplaceCOM} expansion is a center of mass expansion of the Laplace
potential. This form of the expansion extends to the quadrupole term, and
because of the choice of center has an identically zero dipole term. This
expansion can be used to compute the acceleration. See {\tt LaplaceCOM} for an
equivalent expansion that computes the potential. This potential is
scale-invariant so not kernel parameters are needed in the call to 
{\tt evaluate()}. Further, the number of terms in the expansion is fixed, so the
accuracy parameter to {\tt evaluate()} is ignored.

This expansion is only compatible with the {\tt BH} or {\tt Direct} methods; 
it does not implement all the needed operations for use with {\tt FMM} or 
{\tt FMM97}.

This expansion imposes the following restrictions on the source type: a
member of type {\tt Point} with the name {\tt position} must be provided; a
member of  type {\tt double} with the name {\tt charge} must be provided.

This expansion imposes the following restrictions on the target type: a
member of type {\tt Point} with the name {\tt position} must be provided; 
a member of
type {\tt double [3]} with the name {\tt acceleration} must be provided.


\chapter{Advanced guide to DASHMM} 


\end{document}
